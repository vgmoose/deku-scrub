<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lvmcache | Deku Scrub]]></title>
  <link href="//vgmoose.github.io/deku-scrub/blag//blog/categories/lvmcache/atom.xml" rel="self"/>
  <link href="//vgmoose.github.io/deku-scrub/blag//"/>
  <updated>2014-06-15T02:51:31-04:00</updated>
  <id>//vgmoose.github.io/deku-scrub/blag//</id>
  <author>
    <name><![CDATA[Ricky Ayoub]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Day 11: Triple Threat]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/05/day-11-triple-threat/"/>
    <updated>2014-06-05T19:32:13-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/05/day-11-triple-threat</id>
    <content type="html"><![CDATA[<p>Much of my job consists of taking my computers&hellip; and destroying them. Well, I&rsquo;m not supposed to be destroying them, and I&rsquo;m not necessarily trying to either, but usually in testing LVM I manage to find one way or another to make the machines unbootable. Thankfully, due to the wealth of LVM information out there, more often than not I am capable of fixing the problems that develop on my laptop, but for the desktops it&rsquo;s usually just less effort to reformat them and try something different.</p>

<p><img class="center <a" src="href="http://imgs.xkcd.com/comics/computer_problems.png">http://imgs.xkcd.com/comics/computer_problems.png</a>"></p>

<p>I was going to put in that <a href="http://xkcd.com/349/">other xkcd</a> relating to computer problems, but this one seems more appropriate.</p>

<p>Now don&rsquo;t get me wrong, breaking the computers isn&rsquo;t necessarily <em>bad</em>. The actual problem is the fact that usually they break because of something that <em>I</em> do to them that&rsquo;s out of line. But then I learn so even that isn&rsquo;t a lose-lose situation. In reality, if I could find something that the user would expect to work according to the documentation that consistently (this being the key word here) breaks the machine, then I can create a bugzilla report from that.</p>

<p>So today&rsquo;s big news was that I had all three computers down, and was gong back and forth between them all frantically trying to fix them. For the laptop that means making sure I can still boot to my fedora installâ€“- this usually means fixing it from the rhel7 release candidate dual boot. For the other two computers, it&rsquo;s reformatting them and deciding on a new order / seeing what exactly went wrong during the latest install. Although Red Hat has a network boot system with a bunch of system installers on it, I usually find it easier to use my usb with rhel7 on it.</p>

<p>I&rsquo;m pretty picky with bugs, but I am surprised by how it can sometimes be very simple to break the boot order. That being said, I am practically playing with fire here with some of the commands that I run.</p>

<p><code>
mkfs.ext4 /dev/mapper/vg-thinpool
</code></p>

<p>The above command will format the thinpool and at first I was surprised that it worked, but in retrospect I don&rsquo;t know how it could be prevented. Upon running it my thin volumes that were within that pool seemed totally broken (and I can&rsquo;t say that I didn&rsquo;t expect them to be.)</p>

<p>Another caveat that frequently and still catches me is when I&rsquo;m playing around with a ramdisk via loop back device as discussed a few posts ago. If one reboots without running <code>vgreduce</code> to remove the loopback device from the volume group, chaos usually ensues.</p>

<p>When it&rsquo;s all said and done though, hopefully I can find some combination that people haven&rsquo;t accounted for that will destroy the systems. And until then, hopefully I can understand how and why the machine are actually breaking.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 08: Enter the Cache]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache/"/>
    <updated>2014-06-02T18:24:45-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache</id>
    <content type="html"><![CDATA[<p>Logical volumes are very unique in the sense that you can inject some higher level logic into something as low level as the bytes on your disk. Flash disks are very fast, but large ones are also still very expensive. This obviously creates an interesting problem for server maintainers, as ideally large amount of content would be served as fast as possible which would require the best of both worlds.</p>

<p><img class="center" src="/deku-scrub/blag/images/cash.gif"></p>

<p>Not.. that kind of cash. Ha. HA HA. <a href="http://en.wikipedia.org/wiki/Dm-cache">dm-cache</a> was a proprosed kernel module solution to allow quick access to frequently used blocks from a large slow disk using a quick fast disk. As of <a href="https://www.kernel.org/doc/Documentation/device-mapper/cache.txt">April 28, 2013</a> it is now officially a part of the Linux kernel. The Wiki article does a better job of explaining, as usual, but dm-cache helps speed up a slow device by storing only the modified blocks to the fast device and marking the slow device as dirty, allowing full reads and writes to occur solely off the cache. For this reason, it is recommended that the caching device be <a href="http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1">mirrored</a>.</p>

<p>LVM cache, from my understanding, provides an easy approach to allow the user to configure this through LVM by making use of the dm-cache module. At its heart, lvmcache requires an original, large and slow logical volume, and then a smaller but fast logical volume. A third logical volume is also utilized for metadata storage, not unlike how a thinpool stores its metadata.</p>

<p>An odd tidbit about lvmcache, though, is that the logical volumes that are utilized must all be within the same volume group. To review, a physical volume (like an actual primary partition of a device or another device entirely) can only be within one volume group at a time, and the volume group provides an abstraction over those physical devices to the logical volumes. Or at least that&rsquo;s what I thought.</p>

<p><code>
lvcreate -n isolated -L 10G volume_group /dev/sdb1
</code></p>

<p>Yeah, so apparently you can pass a physical volume to the logical volume creation command and it will ensure that all writes go to that device. At least, that&rsquo;s my understanding of it. I&rsquo;m not entirely sure how you&rsquo;d enforce say, a 10G logical volume, if you tied it to a 4G device, and I really don&rsquo;t enjoy how it breaches the abstraction provided by the volume group.</p>

<p>Addtionally I can&rsquo;t seem to be able to figure out just <em>where</em> these devices are listed (they aren&rsquo;t listed in <code>lvdisplay</code>). The process can also be simplified by using tags, which I do know how to display.</p>

<p><code>
lvs -o+tags
</code></p>

<p>But unfortunately, I don&rsquo;t know much else about them. I believe though that they provide some abstraction that was desired previously with the bypassing of the volume group. <a href="http://rwmj.wordpress.com/2014/05/30/lvm-cache-contd-tip-using-tags/">Here is more information</a> on that.</p>
]]></content>
  </entry>
  
</feed>
