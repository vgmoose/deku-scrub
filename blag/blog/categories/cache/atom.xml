<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cache | Deku Scrub]]></title>
  <link href="//vgmoose.github.io/deku-scrub/blag//blog/categories/cache/atom.xml" rel="self"/>
  <link href="//vgmoose.github.io/deku-scrub/blag//"/>
  <updated>2014-06-15T02:25:23-04:00</updated>
  <id>//vgmoose.github.io/deku-scrub/blag//</id>
  <author>
    <name><![CDATA[Ricky Ayoub]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Day 08: Enter the Cache]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache/"/>
    <updated>2014-06-02T18:24:45-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache</id>
    <content type="html"><![CDATA[<p>Logical volumes are very unique in the sense that you can inject some higher level logic into something as low level as the bytes on your disk. Flash disks are very fast, but large ones are also still very expensive. This obviously creates an interesting problem for server maintainers, as ideally large amount of content would be served as fast as possible which would require the best of both worlds.</p>

<p><img class="center" src="/deku-scrub/blag/images/cash.gif"></p>

<p>Not.. that kind of cash. Ha. HA HA. <a href="http://en.wikipedia.org/wiki/Dm-cache">dm-cache</a> was a proprosed kernel module solution to allow quick access to frequently used blocks from a large slow disk using a quick fast disk. As of <a href="https://www.kernel.org/doc/Documentation/device-mapper/cache.txt">April 28, 2013</a> it is now officially a part of the Linux kernel. The Wiki article does a better job of explaining, as usual, but dm-cache helps speed up a slow device by storing only the modified blocks to the fast device and marking the slow device as dirty, allowing full reads and writes to occur solely off the cache. For this reason, it is recommended that the caching device be <a href="http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1">mirrored</a>.</p>

<p>LVM cache, from my understanding, provides an easy approach to allow the user to configure this through LVM by making use of the dm-cache module. At its heart, lvmcache requires an original, large and slow logical volume, and then a smaller but fast logical volume. A third logical volume is also utilized for metadata storage, not unlike how a thinpool stores its metadata.</p>

<p>An odd tidbit about lvmcache, though, is that the logical volumes that are utilized must all be within the same volume group. To review, a physical volume (like an actual primary partition of a device or another device entirely) can only be within one volume group at a time, and the volume group provides an abstraction over those physical devices to the logical volumes. Or at least that&rsquo;s what I thought.</p>

<p><code>
lvcreate -n isolated -L 10G volume_group /dev/sdb1
</code></p>

<p>Yeah, so apparently you can pass a physical volume to the logical volume creation command and it will ensure that all writes go to that device. At least, that&rsquo;s my understanding of it. I&rsquo;m not entirely sure how you&rsquo;d enforce say, a 10G logical volume, if you tied it to a 4G device, and I really don&rsquo;t enjoy how it breaches the abstraction provided by the volume group.</p>

<p>Addtionally I can&rsquo;t seem to be able to figure out just <em>where</em> these devices are listed (they aren&rsquo;t listed in <code>lvdisplay</code>). The process can also be simplified by using tags, which I do know how to display.</p>

<p><code>
lvs -o+tags
</code></p>

<p>But unfortunately, I don&rsquo;t know much else about them. I believe though that they provide some abstraction that was desired previously with the bypassing of the volume group. <a href="http://rwmj.wordpress.com/2014/05/30/lvm-cache-contd-tip-using-tags/">Here is more information</a> on that.</p>
]]></content>
  </entry>
  
</feed>
