<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lvm | Deku Scrub]]></title>
  <link href="//vgmoose.github.io/deku-scrub/blag//blog/categories/lvm/atom.xml" rel="self"/>
  <link href="//vgmoose.github.io/deku-scrub/blag//"/>
  <updated>2014-06-15T02:51:31-04:00</updated>
  <id>//vgmoose.github.io/deku-scrub/blag//</id>
  <author>
    <name><![CDATA[Ricky Ayoub]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Day 11: Triple Threat]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/05/day-11-triple-threat/"/>
    <updated>2014-06-05T19:32:13-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/05/day-11-triple-threat</id>
    <content type="html"><![CDATA[<p>Much of my job consists of taking my computers&hellip; and destroying them. Well, I&rsquo;m not supposed to be destroying them, and I&rsquo;m not necessarily trying to either, but usually in testing LVM I manage to find one way or another to make the machines unbootable. Thankfully, due to the wealth of LVM information out there, more often than not I am capable of fixing the problems that develop on my laptop, but for the desktops it&rsquo;s usually just less effort to reformat them and try something different.</p>

<p><img class="center <a" src="href="http://imgs.xkcd.com/comics/computer_problems.png">http://imgs.xkcd.com/comics/computer_problems.png</a>"></p>

<p>I was going to put in that <a href="http://xkcd.com/349/">other xkcd</a> relating to computer problems, but this one seems more appropriate.</p>

<p>Now don&rsquo;t get me wrong, breaking the computers isn&rsquo;t necessarily <em>bad</em>. The actual problem is the fact that usually they break because of something that <em>I</em> do to them that&rsquo;s out of line. But then I learn so even that isn&rsquo;t a lose-lose situation. In reality, if I could find something that the user would expect to work according to the documentation that consistently (this being the key word here) breaks the machine, then I can create a bugzilla report from that.</p>

<p>So today&rsquo;s big news was that I had all three computers down, and was gong back and forth between them all frantically trying to fix them. For the laptop that means making sure I can still boot to my fedora install–- this usually means fixing it from the rhel7 release candidate dual boot. For the other two computers, it&rsquo;s reformatting them and deciding on a new order / seeing what exactly went wrong during the latest install. Although Red Hat has a network boot system with a bunch of system installers on it, I usually find it easier to use my usb with rhel7 on it.</p>

<p>I&rsquo;m pretty picky with bugs, but I am surprised by how it can sometimes be very simple to break the boot order. That being said, I am practically playing with fire here with some of the commands that I run.</p>

<p><code>
mkfs.ext4 /dev/mapper/vg-thinpool
</code></p>

<p>The above command will format the thinpool and at first I was surprised that it worked, but in retrospect I don&rsquo;t know how it could be prevented. Upon running it my thin volumes that were within that pool seemed totally broken (and I can&rsquo;t say that I didn&rsquo;t expect them to be.)</p>

<p>Another caveat that frequently and still catches me is when I&rsquo;m playing around with a ramdisk via loop back device as discussed a few posts ago. If one reboots without running <code>vgreduce</code> to remove the loopback device from the volume group, chaos usually ensues.</p>

<p>When it&rsquo;s all said and done though, hopefully I can find some combination that people haven&rsquo;t accounted for that will destroy the systems. And until then, hopefully I can understand how and why the machine are actually breaking.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 10: Snapshot Restore Aftermath]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/04/day-10-snopshot-restore-aftermath/"/>
    <updated>2014-06-04T19:31:40-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/04/day-10-snopshot-restore-aftermath</id>
    <content type="html"><![CDATA[<p>So today I thought I&rsquo;d through caution to the wind and just restore one of the daily snapshots that I had lying around from the cron job. This turned out to be not such a great idea&hellip; Despite being in charge of reviewing the documentation, it was not so obvious to me that upon merging one of the snapshots from my &ldquo;Time Machine&rdquo; the other ones would be de-linked as snapshots, and simply exist as thin volumes within the pool. This is pretty upsetting, as the inability to merge the snapshots that I have diminishes their value&hellip; Or so I thought.</p>

<p>You see, as it turns out, and as I probably should have realized, merging thin snapshots isn&rsquo;t the only way to restore one of these snapshots. Since at a lower level device mapper is taking care of the bytes within the thin pool, I can simply rename the snapshot I want to restore&rsquo;s thin volume to my current one and swap them. As I&rsquo;ve been backing up my home directory, I&rsquo;ll obviously have to do this either as root or from another volume, but such was the case with merging as well.</p>

<p><code>
lvrename vg/cur_home old_home
lvrename vg/snap_to_restore cur_home
</code></p>

<p>In addition to that, the skip automatic activation flags should be set up accordingly, and at the very least the new home volume should have it turned off. This is that weird <code>-k</code> stuff.</p>

<p><code>
lvchange -kn vg/cur_home
lvchange -ky vg/old_home
</code></p>

<p>And then on the next reboot all was well&hellip; With the exception of <code>lvs</code> looking a bit messier now (as you can tell when a point has been restored from by looking at the origins).</p>

<p><img class="center" src="/deku-scrub/blag/images/IMG_20140604_162029.jpg"></p>

<p>In other news, I&rsquo;ve received another monitor. Still only one mouse and keyboard, but swapping those isn&rsquo;t nearly as tedious as swapping monitors, so that&rsquo;s pretty useful. Above is my current setup, and below are more pics.</p>

<p><img class="center" src="/deku-scrub/blag/images/IMG_20140604_162037.jpg"></p>

<p>Here&rsquo;s desk space here if I ever need to write anything down. I was considering splaying some papers there to make it look like it was in use, but that would be weird.</p>

<p><img class="center" src="/deku-scrub/blag/images/IMG_20140604_162042.jpg"></p>

<p>I&rsquo;d move the other one to the floor, but until I get an extension cord moving the first one isn&rsquo;t much of an option. This bottom one is the one with the removable hard disks, so in retrospect perhaps I should have left it on the desk instead of having to lean down to remove a disk.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 09: RAM disks and Write Speeds]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/03/day-09-ram-disks-and-write-speeds/"/>
    <updated>2014-06-03T19:31:20-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/03/day-09-ram-disks-and-write-speeds</id>
    <content type="html"><![CDATA[<p>As discussed in yesterday&rsquo;s post, caching is beneficial when a large slow volume is assisted by a fast small one. While it isn&rsquo;t a practical use for production, my manager suggested I could emulate this faster behavior by using a ramdisk, as 2/3 of my machines don&rsquo;t have flash storage. A <a href="http://en.wikipedia.org/wiki/RAM_drive">RAM disk</a>, (which, from this point on, I shall continue going back and forth between &ldquo;RAM disk&rdquo; and &ldquo;ramdisk&rdquo; spellings) allows a piece of memory to be sectioned off and mounted like a disk would. This means it no longer performs it&rsquo;s RAM duties of storing memory, and the computer effectively treats your computer has having that much less RAM.</p>

<p><code>
mkdir /mnt/ramdisk
mount -t tmpfs -o size=4096m tmpfs /mnt/ramdisk
</code></p>

<p>Those commands will create a ramdisk on a linux system. <a href="http://www.tekrevue.com/tip/how-to-create-a-4gbs-ram-disk-in-mac-os-x/">Mac instructions here</a>. You can then test the write speed, and note how much faster the RAM disk reacts than your disk originally had.</p>

<p>```</p>

<h1>dd if=/dev/zero of=/mnt/ramdisk/disk.img bs=1m count=4096</h1>

<p>4194304000 GB transferred in 4.0 GB/s</p>

<h1>dd if=/dev/zero of=~/disk.img bs=1m count=4000</h1>

<p>4194304000 GB transferred in 300 MB/s
```</p>

<p>As seen there, the RAM is much faster. There are a couple of issues of course. The biggest one is when you reboot the ramdisk won&rsquo;t just be unmounted– it will be totally deleted. So you can&rsquo;t really rely on it for anything. In fact, the linux commands I listed above use <a href="http://en.wikipedia.org/wiki/Tmpfs">tmpfs</a>, which is designed to optimize things in such a situation.</p>

<p>There&rsquo;s another problem, though. I wanted to use the RAM disks as physical volumes on which to extend a volume group over to use it with lvmcache. AFAIK, the only way to do this is via a loopback device, as &ldquo;mounting&rdquo; the ramdisk doesn&rsquo;t actually count as a device in <code>/dev</code>, which is required to create a physical volume.</p>

<p><code>
losetup /mnt/ramdisk/disk.img /dev/loop1
</code></p>

<p>And after this, it can be formatted as an LVM2 Physical Volume and added to the volume group.</p>

<p><code>
pvcreate /dev/loop1
vgextend volume_group /dev/loop1
</code></p>

<p>There is a caveat to this though, which has hurt me on a couple of occasions&mdash; since the loopback device is temporary, once the computer reboots the physical device won&rsquo;t exist anymore. Normally that would mean the data on that device wouldn&rsquo;t be accessible, but as that device was part of a volume group, the volume group becomes inconsistent, and won&rsquo;t be able to be mounted until it is properly repaired.</p>

<p>To prevent this, prior to rebooting the following command needs to be executed:</p>

<p><code>
vgreduce volume_group /dev/loop1
</code></p>

<p>And the following command will attempt to backtrack you out of the situation where one forgets to do that prior to it, and needs to recover the volume group to access the logical volumes on it. Not that anyone would <em>ever</em> do that.</p>

<p><code>
vgreduce volumegroup --removemissing
</code></p>

<p>(Yeah, I&rsquo;ve more recorded these for myself to reference at a later date, so what.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 08: Enter the Cache]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache/"/>
    <updated>2014-06-02T18:24:45-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache</id>
    <content type="html"><![CDATA[<p>Logical volumes are very unique in the sense that you can inject some higher level logic into something as low level as the bytes on your disk. Flash disks are very fast, but large ones are also still very expensive. This obviously creates an interesting problem for server maintainers, as ideally large amount of content would be served as fast as possible which would require the best of both worlds.</p>

<p><img class="center" src="/deku-scrub/blag/images/cash.gif"></p>

<p>Not.. that kind of cash. Ha. HA HA. <a href="http://en.wikipedia.org/wiki/Dm-cache">dm-cache</a> was a proprosed kernel module solution to allow quick access to frequently used blocks from a large slow disk using a quick fast disk. As of <a href="https://www.kernel.org/doc/Documentation/device-mapper/cache.txt">April 28, 2013</a> it is now officially a part of the Linux kernel. The Wiki article does a better job of explaining, as usual, but dm-cache helps speed up a slow device by storing only the modified blocks to the fast device and marking the slow device as dirty, allowing full reads and writes to occur solely off the cache. For this reason, it is recommended that the caching device be <a href="http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1">mirrored</a>.</p>

<p>LVM cache, from my understanding, provides an easy approach to allow the user to configure this through LVM by making use of the dm-cache module. At its heart, lvmcache requires an original, large and slow logical volume, and then a smaller but fast logical volume. A third logical volume is also utilized for metadata storage, not unlike how a thinpool stores its metadata.</p>

<p>An odd tidbit about lvmcache, though, is that the logical volumes that are utilized must all be within the same volume group. To review, a physical volume (like an actual primary partition of a device or another device entirely) can only be within one volume group at a time, and the volume group provides an abstraction over those physical devices to the logical volumes. Or at least that&rsquo;s what I thought.</p>

<p><code>
lvcreate -n isolated -L 10G volume_group /dev/sdb1
</code></p>

<p>Yeah, so apparently you can pass a physical volume to the logical volume creation command and it will ensure that all writes go to that device. At least, that&rsquo;s my understanding of it. I&rsquo;m not entirely sure how you&rsquo;d enforce say, a 10G logical volume, if you tied it to a 4G device, and I really don&rsquo;t enjoy how it breaches the abstraction provided by the volume group.</p>

<p>Addtionally I can&rsquo;t seem to be able to figure out just <em>where</em> these devices are listed (they aren&rsquo;t listed in <code>lvdisplay</code>). The process can also be simplified by using tags, which I do know how to display.</p>

<p><code>
lvs -o+tags
</code></p>

<p>But unfortunately, I don&rsquo;t know much else about them. I believe though that they provide some abstraction that was desired previously with the bypassing of the volume group. <a href="http://rwmj.wordpress.com/2014/05/30/lvm-cache-contd-tip-using-tags/">Here is more information</a> on that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 07: Daily Time Machine]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/05/30/day-07-daily-time-machine/"/>
    <updated>2014-05-30T22:17:38-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/05/30/day-07-daily-time-machine</id>
    <content type="html"><![CDATA[<p>Happy Friday! I finally feel like I&rsquo;m becoming well-versed with these LVM commands. I&rsquo;m also starting to feel a lot more comfortable in general. It might just be because we&rsquo;re using Thinkpads, but this laptop has byfar been the most comfortable linux experience I&rsquo;ve had to date. There are still very minor things that are preventing me from using it all the time (some as major as the trackpad being awful, and some as minor as the default gnome drop shadow size) but it is all around very pleasant. Maybe I&rsquo;m being brainwashed by Red Hat, but I&rsquo;m also probably gonna swap out my home server with Fedora instead of Debian.</p>

<p>But that&rsquo;s ALL besides the point. Today I set up a cool thing which I shall now share with you now.</p>

<p>```</p>

<h1>lvs</h1>

<p>  LV            VG    Attr       LSize  Pool    Origin Data%  Meta%
  homey         purin Vwi-aotz&mdash;  1.00t thinset          2.24   <br/>
  &hellip;
  snap05.30.14e purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  snap05.30.14m purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  snap06.01.14e purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  thinset       purin twi-a-tz&mdash; 50.00g                 47.15  25.06
```</p>

<p>Here you can see that I have a 50G lvm thin pool called &ldquo;thinset&rdquo; (at the bottom), and within this pool is a logical thin volume called &ldquo;homey&rdquo;. Then there are three snapshots that use &ldquo;homey&rdquo; as their origin, meaning they are a snapshot of &ldquo;homey,&rdquo; and will copy the required bytes on write to maintain their snapshot state.</p>

<p>What ISN&rsquo;T shown here is that I currently have the &ldquo;homey&rdquo; LV mounted at <code>/home</code></p>

<p>```</p>

<h1>df -h</h1>

<p>Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/purin-homey 1008G  6.0G  951G   1% /home
```</p>

<p>There are the usual interesting things about lvmthin volumes that you can note here, such as <code>df</code> believing the filesystem only has 1% of its &ldquo;1008G&rdquo; used, when in reality the thin pool only has 50G that it is able to allocated. But you can also see that I have snapshots of the form <code>snapMM.DD.YY(e/m)</code>, which are taken daily on that date (with e for evening and m for morning). This was configured via <code>cron</code> by editing <code>/etc/crontab</code> to have the following new lines appended:</p>

<p><code>
  0  10 *  *  * root       lvcreate -s purin/homey -n snap$(date +\%m.\%d.\%y)m
  0  17 *  *  * root       lvcreate -s purin/homey -n snap$(date +\%m.\%d.\%y)e
</code></p>

<p>Making my snapshot commands being run on the 10th (10AM) and 17th (5PM) hours of each day. It&rsquo;s also worth noting that <code>anacron</code> could be used if I wanted to queue up the snapshots for when the computer was powered down, but that isn&rsquo;t desired behavior, at least not for me.</p>

<p>You might be thinking to yourself &ldquo;Okay who cares about cron what does that really have to do with your internship?&rdquo; To which I respond it&rsquo;s just cool, okay?! SHARING IS CARING.</p>

<p>Some more things to note here is that I did have to create the &ldquo;homey&rdquo; LV from nothing and manually copy over my files from my old <code>/home</code>. This is due to the fact that I needed it to be within a thin pool, and you can only take thin snapshots of an external origin (LV that isn&rsquo;t within a thin pool) when that source is read-only. I thought I&rsquo;d just start it all over.</p>

<p><code>
lvconvert --merge purin/snapMM.DD.YYz
</code></p>

<p>The above command can be run (when <code>/home</code> isn&rsquo;t mounted) to rollback to the given snapshot day. I&rsquo;ve tested it a little bit so far and it has been successful, but it should get a lot more interesting once I have a bunch of snapshots to go off. If one tries to <code>lvconvert</code> while the origin is mounted, the merge will be queued until the next mount of the <strong>snapshot</strong>. This actually took me a while to learn. And don&rsquo;t forget, to activate a thin snapshot one needs to pass the <code>-K</code> flag as mentioned in an earlier blog post.</p>

<p>I have a couple more questions in my head about this mysterious, magical snapshot process. The big ones are: What happens to garbage files, like if I were to run <code>dd if=/dev/zero of=file.out bs=4k count=5000000</code>? Would the resulting 10G file be kept around in the thinpool? And if so, how could that easily be detached from all the snapshots? And another question is, in the output from <code>lvs</code> above, why is the data percent so high? I am not using 2% of 1T, as seen in <code>df</code> my home directory is only 6G, and 2% of 1T is 20G. Is that <em>because</em> I ran commands similar to that <code>dd</code> one? Not that I <em>did</em>.</p>

<p>I have a feeling Monday&rsquo;s post is going to involve ram disks and dm-cache, so stay tuned and try not to fall off the edge of your seat.</p>
]]></content>
  </entry>
  
</feed>
