<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: lvm | Deku Scrub]]></title>
  <link href="//vgmoose.github.io/deku-scrub/blag//blog/categories/lvm/atom.xml" rel="self"/>
  <link href="//vgmoose.github.io/deku-scrub/blag//"/>
  <updated>2014-06-13T00:33:37-04:00</updated>
  <id>//vgmoose.github.io/deku-scrub/blag//</id>
  <author>
    <name><![CDATA[Ricky Ayoub]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Day 09: RAM disks and Write Speeds]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/03/day-09-ram-disks-and-write-speeds/"/>
    <updated>2014-06-03T19:31:20-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/03/day-09-ram-disks-and-write-speeds</id>
    <content type="html"><![CDATA[<p>As discussed in yesterday&rsquo;s post, caching is beneficial when a large slow volume is assisted by a fast small one. While it isn&rsquo;t a practical use for production, my manager suggested I could emulate this faster behavior by using a ramdisk, as 2/3 of my machines don&rsquo;t have flash storage. A <a href="http://en.wikipedia.org/wiki/RAM_drive">RAM disk</a>, (which, from this point on, I shall continue going back and forth between &ldquo;RAM disk&rdquo; and &ldquo;ramdisk&rdquo; spellings) allows a piece of memory to be sectioned off and mounted like a disk would. This means it no longer performs it&rsquo;s RAM duties of storing memory, and the computer effectively treats your computer has having that much less RAM.</p>

<p><code>
mkdir /mnt/ramdisk
mount -t tmpfs -o size=4096m tmpfs /mnt/ramdisk
</code></p>

<p>Those commands will create a ramdisk on a linux system. <a href="http://www.tekrevue.com/tip/how-to-create-a-4gbs-ram-disk-in-mac-os-x/">Mac instructions here</a>. You can then test the write speed, and note how much faster the RAM disk reacts than your disk originally had.</p>

<p>```</p>

<h1>dd if=/dev/zero of=/mnt/ramdisk/disk.img bs=1m count=4096</h1>

<p>4194304000 GB transferred in 4.0 GB/s</p>

<h1>dd if=/dev/zero of=~/disk.img bs=1m count=4000</h1>

<p>4194304000 GB transferred in 300 MB/s
```</p>

<p>As seen there, the RAM is much faster. There are a couple of issues of course. The biggest one is when you reboot the ramdisk won&rsquo;t just be unmountedâ€“ it will be totally deleted. So you can&rsquo;t really rely on it for anything. In fact, the linux commands I listed above use <a href="http://en.wikipedia.org/wiki/Tmpfs">tmpfs</a>, which is designed to optimize things in such a situation.</p>

<p>There&rsquo;s another problem, though. I wanted to use the RAM disks as physical volumes on which to extend a volume group over to use it with lvmcache. AFAIK, the only way to do this is via a loopback device, as &ldquo;mounting&rdquo; the ramdisk doesn&rsquo;t actually count as a device in <code>/dev</code>, which is required to create a physical volume.</p>

<p><code>
losetup /mnt/ramdisk/disk.img /dev/loop1
</code></p>

<p>And after this, it can be formatted as an LVM2 Physical Volume and added to the volume group.</p>

<p><code>
pvcreate /dev/loop1
vgextend volume_group /dev/loop1
</code></p>

<p>There is a caveat to this though, which has hurt me on a couple of occasions&mdash; since the loopback device is temporary, once the computer reboots the physical device won&rsquo;t exist anymore. Normally that would mean the data on that device wouldn&rsquo;t be accessible, but as that device was part of a volume group, the volume group becomes inconsistent, and won&rsquo;t be able to be mounted until it is properly repaired.</p>

<p>To prevent this, prior to rebooting the following command needs to be executed:</p>

<p><code>
vgreduce volume_group /dev/loop1
</code></p>

<p>And the following command will attempt to backtrack you out of the situation where one forgets to do that prior to it, and needs to recover the volume group to access the logical volumes on it. Not that anyone would <em>ever</em> do that.</p>

<p><code>
vgreduce volumegroup --removemissing
</code></p>

<p>(Yeah, I&rsquo;ve more recorded these for myself to reference at a later date, so what.)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 08: Enter the Cache]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache/"/>
    <updated>2014-06-02T18:24:45-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/06/02/day-08-enter-the-cache</id>
    <content type="html"><![CDATA[<p>Logical volumes are very unique in the sense that you can inject some higher level logic into something as low level as the bytes on your disk. Flash disks are very fast, but large ones are also still very expensive. This obviously creates an interesting problem for server maintainers, as ideally large amount of content would be served as fast as possible which would require the best of both worlds.</p>

<p><img class="center" src="/deku-scrub/blag/images/cash.gif"></p>

<p>Not.. that kind of cash. Ha. HA HA. <a href="http://en.wikipedia.org/wiki/Dm-cache">dm-cache</a> was a proprosed kernel module solution to allow quick access to frequently used blocks from a large slow disk using a quick fast disk. As of <a href="https://www.kernel.org/doc/Documentation/device-mapper/cache.txt">April 28, 2013</a> it is now officially a part of the Linux kernel. The Wiki article does a better job of explaining, as usual, but dm-cache helps speed up a slow device by storing only the modified blocks to the fast device and marking the slow device as dirty, allowing full reads and writes to occur solely off the cache. For this reason, it is recommended that the caching device be <a href="http://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_1">mirrored</a>.</p>

<p>LVM cache, from my understanding, provides an easy approach to allow the user to configure this through LVM by making use of the dm-cache module. At its heart, lvmcache requires an original, large and slow logical volume, and then a smaller but fast logical volume. A third logical volume is also utilized for metadata storage, not unlike how a thinpool stores its metadata.</p>

<p>An odd tidbit about lvmcache, though, is that the logical volumes that are utilized must all be within the same volume group. To review, a physical volume (like an actual primary partition of a device or another device entirely) can only be within one volume group at a time, and the volume group provides an abstraction over those physical devices to the logical volumes. Or at least that&rsquo;s what I thought.</p>

<p><code>
lvcreate -n isolated -L 10G volume_group /dev/sdb1
</code></p>

<p>Yeah, so apparently you can pass a physical volume to the logical volume creation command and it will ensure that all writes go to that device. At least, that&rsquo;s my understanding of it. I&rsquo;m not entirely sure how you&rsquo;d enforce say, a 10G logical volume, if you tied it to a 4G device, and I really don&rsquo;t enjoy how it breaches the abstraction provided by the volume group.</p>

<p>Addtionally I can&rsquo;t seem to be able to figure out just <em>where</em> these devices are listed (they aren&rsquo;t listed in <code>lvdisplay</code>). The process can also be simplified by using tags, which I do know how to display.</p>

<p><code>
lvs -o+tags
</code></p>

<p>But unfortunately, I don&rsquo;t know much else about them. I believe though that they provide some abstraction that was desired previously with the bypassing of the volume group. <a href="http://rwmj.wordpress.com/2014/05/30/lvm-cache-contd-tip-using-tags/">Here is more information</a> on that.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 07: Daily Time Machine]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/05/30/day-07-daily-time-machine/"/>
    <updated>2014-05-30T22:17:38-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/05/30/day-07-daily-time-machine</id>
    <content type="html"><![CDATA[<p>Happy Friday! I finally feel like I&rsquo;m becoming well-versed with these LVM commands. I&rsquo;m also starting to feel a lot more comfortable in general. It might just be because we&rsquo;re using Thinkpads, but this laptop has byfar been the most comfortable linux experience I&rsquo;ve had to date. There are still very minor things that are preventing me from using it all the time (some as major as the trackpad being awful, and some as minor as the default gnome drop shadow size) but it is all around very pleasant. Maybe I&rsquo;m being brainwashed by Red Hat, but I&rsquo;m also probably gonna swap out my home server with Fedora instead of Debian.</p>

<p>But that&rsquo;s ALL besides the point. Today I set up a cool thing which I shall now share with you now.</p>

<p>```</p>

<h1>lvs</h1>

<p>  LV            VG    Attr       LSize  Pool    Origin Data%  Meta%
  homey         purin Vwi-aotz&mdash;  1.00t thinset          2.24   <br/>
  &hellip;
  snap05.30.14e purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  snap05.30.14m purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  snap06.01.14e purin Vwi&mdash;-tz-k  1.00t thinset homey           <br/>
  thinset       purin twi-a-tz&mdash; 50.00g                 47.15  25.06
```</p>

<p>Here you can see that I have a 50G lvm thin pool called &ldquo;thinset&rdquo; (at the bottom), and within this pool is a logical thin volume called &ldquo;homey&rdquo;. Then there are three snapshots that use &ldquo;homey&rdquo; as their origin, meaning they are a snapshot of &ldquo;homey,&rdquo; and will copy the required bytes on write to maintain their snapshot state.</p>

<p>What ISN&rsquo;T shown here is that I currently have the &ldquo;homey&rdquo; LV mounted at <code>/home</code></p>

<p>```</p>

<h1>df -h</h1>

<p>Filesystem               Size  Used Avail Use% Mounted on
/dev/mapper/purin-homey 1008G  6.0G  951G   1% /home
```</p>

<p>There are the usual interesting things about lvmthin volumes that you can note here, such as <code>df</code> believing the filesystem only has 1% of its &ldquo;1008G&rdquo; used, when in reality the thin pool only has 50G that it is able to allocated. But you can also see that I have snapshots of the form <code>snapMM.DD.YY(e/m)</code>, which are taken daily on that date (with e for evening and m for morning). This was configured via <code>cron</code> by editing <code>/etc/crontab</code> to have the following new lines appended:</p>

<p><code>
  0  10 *  *  * root       lvcreate -s purin/homey -n snap$(date +\%m.\%d.\%y)m
  0  17 *  *  * root       lvcreate -s purin/homey -n snap$(date +\%m.\%d.\%y)e
</code></p>

<p>Making my snapshot commands being run on the 10th (10AM) and 17th (5PM) hours of each day. It&rsquo;s also worth noting that <code>anacron</code> could be used if I wanted to queue up the snapshots for when the computer was powered down, but that isn&rsquo;t desired behavior, at least not for me.</p>

<p>You might be thinking to yourself &ldquo;Okay who cares about cron what does that really have to do with your internship?&rdquo; To which I respond it&rsquo;s just cool, okay?! SHARING IS CARING.</p>

<p>Some more things to note here is that I did have to create the &ldquo;homey&rdquo; LV from nothing and manually copy over my files from my old <code>/home</code>. This is due to the fact that I needed it to be within a thin pool, and you can only take thin snapshots of an external origin (LV that isn&rsquo;t within a thin pool) when that source is read-only. I thought I&rsquo;d just start it all over.</p>

<p><code>
lvconvert --merge purin/snapMM.DD.YYz
</code></p>

<p>The above command can be run (when <code>/home</code> isn&rsquo;t mounted) to rollback to the given snapshot day. I&rsquo;ve tested it a little bit so far and it has been successful, but it should get a lot more interesting once I have a bunch of snapshots to go off. If one tries to <code>lvconvert</code> while the origin is mounted, the merge will be queued until the next mount of the <strong>snapshot</strong>. This actually took me a while to learn. And don&rsquo;t forget, to activate a thin snapshot one needs to pass the <code>-K</code> flag as mentioned in an earlier blog post.</p>

<p>I have a couple more questions in my head about this mysterious, magical snapshot process. The big ones are: What happens to garbage files, like if I were to run <code>dd if=/dev/zero of=file.out bs=4k count=5000000</code>? Would the resulting 10G file be kept around in the thinpool? And if so, how could that easily be detached from all the snapshots? And another question is, in the output from <code>lvs</code> above, why is the data percent so high? I am not using 2% of 1T, as seen in <code>df</code> my home directory is only 6G, and 2% of 1T is 20G. Is that <em>because</em> I ran commands similar to that <code>dd</code> one? Not that I <em>did</em>.</p>

<p>I have a feeling Monday&rsquo;s post is going to involve ram disks and dm-cache, so stay tuned and try not to fall off the edge of your seat.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 06: Life at Red Hat]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/05/29/day-06-of-bugzilla-and-bagels/"/>
    <updated>2014-05-29T23:42:49-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/05/29/day-06-of-bugzilla-and-bagels</id>
    <content type="html"><![CDATA[<p>This post is focused primarily on what it&rsquo;s like to work at Red Hat. My past few posts have been more on the technical side, as much of what I do consists of that. The other interns have more frequent meetings with their managers, but I appear to be on a much smaller team. As a result of this, much of my day is the same with the <em>exception</em> of the technical stuff that I&rsquo;m playing around with that day.</p>

<p><img class="center" src="/deku-scrub/blag/images/redhat-ovs-sdn.jpg"></p>

<p>Before I get into the actual workflow, HERE&rsquo;S WHAT YOU NEED TO KNOW. I hadn&rsquo;t realized how much I agreed with the &ldquo;ideals&rdquo; of the company until I actually started working there. A couple of years ago if someone said &ldquo;Red Hat&rdquo; I might have responded with &ldquo;oh, those dudes that are trying to commercialize Linux?&rdquo; AND OH HOW WRONG I WOULD HAVE BEEN. I guess the biggest and most important thing that I hadn&rsquo;t realized about the company was that Fedora pretty much IS RHEL (Red Hat Enterprise Linux), or at least acts as its upstream. Fedora is on a 6 month release cycle, while RHEL is on a 2 year release cycle. I had originally thought Red Hat &ldquo;claimed&rdquo; to give back to to the community through Fedora, but put all their good stuff in RHEL. In reality, they actually work <a href="https://git.fedorahosted.org/git/lvm2.git">totally in the open</a>, and then add the finished product to the next RHEL release if it goes over well in Fedora.</p>

<p><img class="center" src="/deku-scrub/blag/images/Justice_League_break_room.jpg"></p>

<p>I don&rsquo;t know if it&rsquo;s the aforementioned culture or what but everyone here is just VERY laid back. Everybody&rsquo;s smiling at you, and nobody seems to throw you a look that says &ldquo;shouldn&rsquo;t you be doing something important instead of playing around on a computer all day?&rdquo; What&rsquo;s more, everyone in the company also unanimously seems to agree that they love it. I never see managers asking people to see their work, and I even overheard a story about a guy who they noticed never came in (always worked from home) so they just gave his cubicle to someone else. It&rsquo;s VERY relaxed. I bought a bunch of semi-fancy shirts only to find that everyone just wears jeans. I REALLY dig the atmosphere. You can tell because I&rsquo;m using the word &ldquo;dig&rdquo; to describe just how much I dig it.</p>

<p>There is a break room on every floor which contains snacks, fruit, coffee, and (most importantly) water. In addition to that, every single morning there are bagels just straight up available with cream cheese. Other than that, though, my daily routine consists of mostly bouncing back and forth between the break room and my cubicle to (fill up on water) and (read documentation || pound my head against some linux thing) respectively.</p>

<p><img class="center" src="/deku-scrub/blag/images/Redhat_NB5.jpg"></p>

<p>When lunch time rolls around, so far I&rsquo;ve only ever eaten at the cafeteria on the first floor (which is pictured above&mdash; it looks really nice because they just redid it). Usually, as mentioned before, I get the grilled chicken sandwich, although today I opted for a turkey wrap. The selection isn&rsquo;t too great, but fortunately what they do have are foods that I eat. There are two main issues that I currently have with lunch: 1. I don&rsquo;t get paid to do it and 2. I have to pay to do it. Even though each meal runs me about $5, it still doesn&rsquo;t feel great to see the money leaving my card balance. What especially doesn&rsquo;t feel great is the fact that if I had greater self control I could probably just tuck away a morning bagel and have that for free.</p>

<p>Sometimes at the end of the day one of the two people on my team (my manager and pseudo-manager) will come up to me and meet with me for a bit and discuss my progress and what I should be looking at and focusing on. Thus far, I&rsquo;ve still just been focusing on the lvm stuff and primarily the lvmthin stuff. There is talk about having me take a look at dm-cache though. With any luck, I should be contributing semi-meaningful <a href="https://bugzilla.redhat.com/buglist.cgi?component=lvm2&amp;product=Red%20Hat%20Enterprise%20Linux%205">bug reports</a> soon.</p>

<p>Other than that, there isn&rsquo;t much going on outside of the technical things discussed here. Once a week all the interns get together and have a group lunch (which we get paid for) and there is talk about a final intern group presentation that we would have to coordinate. Additionally, while I&rsquo;m at work and connected to the intranet, I hang out (but currently do not talk in) IRC channels for lvm and the interns.</p>

<p>Drive there, park, scan inside, eat, cube, drink, cube, drink, cube, eat, drink, cube, drink, cube, talk to manager (sometimes), leave, drive home. And I love it.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Day 05: Thinly Provisioned Snapshots]]></title>
    <link href="//vgmoose.github.io/deku-scrub/blag//blog/2014/05/28/day-05-thinly-provisioned-snapshots/"/>
    <updated>2014-05-28T22:54:34-04:00</updated>
    <id>//vgmoose.github.io/deku-scrub/blag//blog/2014/05/28/day-05-thinly-provisioned-snapshots</id>
    <content type="html"><![CDATA[<p>In yesterday&rsquo;s post I introduced LVM snapshots and why they are useful. It may have gotten lost a bit in the jumble of comamnds. To recap, snapshots <em>physically</em> store only the differences between a filesystem at a given time and the present. Using the differences (which of course are much smaller than the entire filesystem) one can mount and reconstruct the filesystem from the point when the snapshot was created.</p>

<p>That, of course, is really cool, but let&rsquo;s say you want to take a TON of snapshots (like we&rsquo;re talking daily ones). The daily snaps would allow you to easily revert to what the filesystem looked like on a given day without using much more space. Of course, if they were just regular LVM snapshots, you&rsquo;d need to decide on a size to give them. Taking daily snapshots in this manner would fill up in no time.</p>

<p><img class="center" src="/deku-scrub/blag/images/uglydiagram.png"></p>

<p>A thin pool is a logical volume. You give it a set size, and it is that size, much like a logical volume. The total amount of bytes cannot exceed that size ever, and if it does you&rsquo;ll have to resize it. Making one is as follows:</p>

<p><code>
lvcreate --thinpool vg/thin_pool --size 30G
</code></p>

<p>Where vg is the volume group. Nothing special <em>so far</em>. What&rsquo;s amazing though, is within this pool can exist filesystems that just <strong>straight up lie</strong> about how much space they have left. See the 100TB volume within the pool in the diagram above. You can mount that logical volume and it will appear to the computer as a 100TB volume. To create it within <code>thin_pool</code>, issue the following commands:</p>

<p><code>
lvcreate -T vg/thin_pool -V 100T -n big_filesystem
</code></p>

<p>Note the use of the <code>-V</code>  argument for virtual size. Why is this useful? It allows for the volume to dynamically take up space as it needs. Let&rsquo;s say you put 2GB worth of files on this &ldquo;100TB filesystem.&rdquo; Physically, only 2GB worth of data will be <strong>allocated</strong>. Meaning, you can continue to create logical volumes within the pool that will expand up to their maximum virtual size. So this time, when we create our snapshots, their space doesn&rsquo;t matter! They will dynamically expand within the pool to the necessary size (which will be the size of the differences between them in the original). Obviously the <a href="https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Logical_Volume_Manager_Administration/thinly-provisioned_snapshot_volumes.html">official documentation</a> does a better job of explaining this, but I thought I&rsquo;d give it a try.</p>

<p><code>
lvcreate -s vg/big_filesystem -n snappy
</code></p>

<p>Another added bonus of being within the pool is that the thin snapshots will share common data with <em>each other</em> too. This would have also been wasted duplicated space. These enhancements change the game of snapshots from a useful way to get a O(1) image of the filesystem to a meaningful way of storing the filesystem at any point and time.</p>

<p>QUICK CAVEAT. I got hung up on this for a long time. Thin snapshots have a special case when they are generated and are <strong>not</strong> automatically marked active to be mountable in <code>/dev</code>. You can view this information using <code>lvscan</code>. You will see any thin snapshots are marked inactive. Normally to mark a volume active, you&rsquo;d simply perform:</p>

<p><code>
lvconvert -ay vg/lv
</code></p>

<p>However, as thin snapshots have the special flag set to not be marked active, you need to modify this to look like:</p>

<p><code>
lvconvert -ay -K vg/lv
</code></p>

<p>That simple <code>-K</code> argument drove me crazy for a bit. It was mentioned in the documentation, but I must have passed by it. As leaving <code>-K</code> out silently fails, I wasn&rsquo;t sure if my build of lvm2 was broken or what (as it would just not mark it active without telling me it failed).</p>

<p>With this knowledge it is possible to easily create daily full snapshots of an entire filesystem without taking up any more space than would have been taken up by the difference bytes. The thin volumes will dynammically expand to fill the pool, so one does not have to worry at all about the sizes of the snapshots (in fact, you aren&rsquo;t allowed to set it for a thin snapshot). The <a href="http://wiki.gentoo.org/wiki/LVM">Gentoo Wiki</a> actually has really good instructions and easy to follow commands on all of this if you don&rsquo;t want to take my word for them (or in the more likely case, if i&rsquo;ve left something out).</p>
]]></content>
  </entry>
  
</feed>
